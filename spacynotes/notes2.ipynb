{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains notes about **word embeddings and their use cases**, **pipelines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def load_nlp(modelname):\n",
    "    return spacy.load(modelname)\n",
    "\n",
    "nlp = load_nlp(\"en_core_web_lg\")\n",
    "\n",
    "with open (\"/home/eric/python/notes/spacynotes/fcc_repo/data/wiki_us.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    \n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "`Word embeddings` or word vectors, are numerical representations of words in multidimensional space through matrices.\n",
    "\n",
    "The **purpose of the word vector** is to get a computer system to understand a word. \n",
    "\n",
    "These `embeddings` are trained through machine learning mechanisms and attempt to capture **syntactic structure** and **semantic similarity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.2681e+00, -8.5717e-01,  5.8105e+00,  1.9771e+00,  8.8147e+00,\n",
       "       -5.8579e+00,  3.7143e+00,  3.5850e+00,  4.7987e+00, -4.4251e+00,\n",
       "        1.7461e+00, -3.7296e+00, -5.1407e+00, -1.0792e+00, -2.5555e+00,\n",
       "        3.0755e+00,  5.0141e+00,  5.8525e+00,  7.3378e+00, -2.7689e+00,\n",
       "       -5.1641e+00, -1.9879e+00,  2.9782e+00,  2.1024e+00,  4.4306e+00,\n",
       "        8.4355e-01, -6.8742e+00, -4.2949e+00, -1.7294e-01,  3.6074e+00,\n",
       "        8.4379e-01,  3.3419e-01, -4.8147e+00,  3.5683e-02, -1.3721e+01,\n",
       "       -4.6528e+00, -1.4021e+00,  4.8342e-01,  1.2549e+00, -4.0644e+00,\n",
       "        3.3278e+00, -2.1590e-01, -5.1786e+00,  3.5360e+00, -3.1575e+00,\n",
       "       -3.5273e+00, -3.6753e+00,  1.5863e+00, -8.1594e+00, -3.4657e+00,\n",
       "        1.5262e+00,  4.8135e+00, -3.8428e+00, -3.9082e+00,  6.7549e-01,\n",
       "       -3.5787e-01, -1.7806e+00,  3.5284e+00, -5.1114e-02, -9.7150e-01,\n",
       "       -9.0553e-01, -1.5570e+00,  1.2038e+00,  4.7708e+00,  9.8561e-01,\n",
       "       -2.3186e+00, -7.4899e+00, -9.5389e+00,  8.5572e+00,  2.7420e+00,\n",
       "       -3.6270e+00,  2.7456e+00, -6.9574e+00, -1.7190e+00, -2.9145e+00,\n",
       "        1.1838e+00,  3.7864e+00,  2.0413e+00, -3.5808e+00,  1.4319e+00,\n",
       "        2.0528e-01, -7.0640e-01, -5.3556e+00, -2.5911e+00,  4.4922e+00,\n",
       "        1.6574e+00,  3.9794e+00, -4.3560e+00, -2.7266e+00,  1.9581e+00,\n",
       "       -3.4842e+00, -3.9674e+00,  3.2690e+00,  6.6683e-01,  3.9837e+00,\n",
       "       -6.5997e+00,  4.1630e+00,  8.0338e+00,  3.8102e-01,  8.2656e+00,\n",
       "        9.7061e-01, -5.0807e+00,  4.9522e+00,  7.5018e+00,  3.8305e+00,\n",
       "       -3.3233e+00,  4.9126e+00,  2.4189e-01,  3.8218e+00, -3.9717e+00,\n",
       "        2.4691e+00,  1.3721e+01, -8.9664e+00,  1.0610e+01,  6.9425e-01,\n",
       "       -1.1082e+01, -5.6883e+00,  2.3287e+00,  1.6451e+00,  3.6006e+00,\n",
       "        1.2588e-01, -6.1956e+00,  1.1455e+01,  5.6682e+00, -5.0251e-01,\n",
       "       -9.8515e-01,  8.8902e-02, -4.0213e+00,  3.6134e+00, -9.0936e+00,\n",
       "       -1.4555e+01, -2.5591e+00,  4.0959e+00, -3.5929e-01,  1.0219e+00,\n",
       "        3.9402e+00,  8.0495e-01, -3.6023e+00,  2.6394e+00, -1.5258e-01,\n",
       "       -2.6182e+00, -2.6268e-01, -2.1610e+00,  2.3950e+00,  6.8842e+00,\n",
       "        3.6034e+00,  1.8058e+00,  2.4528e+00,  4.4088e+00, -1.0598e+00,\n",
       "        6.4964e+00,  5.9196e+00, -1.0261e+00, -1.7013e+00, -4.4151e+00,\n",
       "        4.3043e+00, -1.7138e+00, -4.6690e+00, -5.5212e-01,  5.3995e+00,\n",
       "        1.8311e+00, -3.5820e-01, -3.6578e-01, -2.8578e+00, -6.4639e+00,\n",
       "       -3.2155e+00,  6.7083e-01, -1.2800e+00,  1.2782e+00,  7.8274e-01,\n",
       "        1.9839e-01, -1.4163e+00,  2.1184e+00,  1.5021e+00, -1.8212e+00,\n",
       "        1.6629e+00,  4.0354e+00, -4.4648e+00, -3.4897e+00, -2.5765e+00,\n",
       "       -3.6317e+00, -4.1619e-02,  4.8660e-01,  2.0712e+00, -1.9166e+00,\n",
       "       -3.4045e+00, -7.6609e+00, -2.1940e+00, -2.3919e-03,  8.4900e-01,\n",
       "        1.3921e+00, -5.7830e+00,  4.4739e+00,  1.0642e+00,  5.7864e+00,\n",
       "        3.4643e+00, -5.9169e+00, -2.6925e+00, -1.1271e-01, -6.0462e+00,\n",
       "        3.9285e+00, -3.0423e+00, -6.9939e-02,  2.2826e-01,  8.0214e+00,\n",
       "        2.2098e+00, -1.1049e+01,  7.6001e-02, -1.5970e+00,  2.0524e-01,\n",
       "        2.8063e+00,  3.5245e+00, -3.9300e+00, -9.7995e-01,  4.0248e+00,\n",
       "        1.8447e+00, -2.0452e+00,  1.1419e+00, -4.4600e-01, -9.5551e-01,\n",
       "       -1.0224e+00,  5.9224e+00, -6.1688e+00, -8.3840e-01, -7.9102e+00,\n",
       "       -8.9575e-02, -2.7741e-01,  4.2703e+00,  4.0212e+00, -1.1166e-01,\n",
       "        2.5119e+00, -5.9635e+00, -1.2320e+00,  2.8199e-01, -4.1062e+00,\n",
       "       -6.2923e-01, -5.2420e-01,  2.5213e+00, -3.5094e+00,  6.4333e+00,\n",
       "        7.9466e+00, -3.3883e+00,  5.2535e+00,  9.4524e-02, -3.3336e+00,\n",
       "        5.9621e+00, -1.0794e+00, -6.0850e+00, -3.6071e+00, -3.8496e-01,\n",
       "        7.6137e+00, -9.1081e+00, -6.0037e+00, -2.4735e+00, -6.5050e-01,\n",
       "       -6.3021e+00,  8.5783e+00,  1.7250e-01,  4.3631e+00, -9.3439e+00,\n",
       "        2.0984e-01,  7.6900e-01,  1.0763e+01,  4.4598e-01, -3.6584e+00,\n",
       "       -3.0992e+00, -3.8868e+00,  4.3337e+00, -5.8037e+00, -1.1337e+00,\n",
       "       -6.1562e+00,  3.1820e-01, -1.0612e+00, -1.4809e+00,  6.0373e+00,\n",
       "        4.6015e-01, -1.5530e+00, -1.0562e+00,  5.8618e-01,  3.4431e+00,\n",
       "        4.5542e+00, -3.1881e+00, -1.5832e+00,  3.0859e+00,  1.3061e+00,\n",
       "       -8.0091e+00,  7.7996e+00, -5.0644e+00,  8.8719e+00,  7.2337e-01,\n",
       "       -1.2350e+00,  1.6209e+00,  7.8994e+00,  1.0741e+01,  8.1158e-01,\n",
       "        9.0156e+00, -1.5913e+00, -5.3166e+00,  3.5032e-01, -2.8850e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = list(doc.sents)[0]\n",
    "\n",
    "sentence1[0].vector # grab the vector of the first word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of these `word embeddings` is to capture similarity quickly and reliably "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'dogs', 'cat', 'puppy', 'pet', 'pup', 'canine', 'wolfdogs', 'dogsled', 'uppy']\n"
     ]
    }
   ],
   "source": [
    "your_word = \"dog\"\n",
    "\n",
    "# finds words similar to your_word\n",
    "ms = nlp.vocab.vectors.most_similar(np.asarray([nlp.vocab.vectors[nlp.vocab.strings[your_word]]]), n=10)\n",
    "words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "distances = ms[2]\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using spacy, we can calculate **simiarity between two docs**, as well as **similarity between two words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like salty fries and hamburgers. <-> Fast food tastes very good. 0.691649353055761\n",
      "salty fries <-> hamburgers 0.6938489675521851\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = load_nlp(\"en_core_web_md\")\n",
    "\n",
    "doc1 = nlp(\"I like salty fries and hamburgers.\")\n",
    "doc2 = nlp(\"Fast food tastes very good.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
    "\n",
    "# Similarity of tokens and spans\n",
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "A **`pipeline`** is a sequence of `pipes`, or actors on data, that **make alterations to the data** or **extract information from it**.\n",
    "\n",
    "<img src=\"pipeline.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "## How to add pipes\n",
    "\n",
    "Sometimes, an `off the shelf pipeline`  from spacy is not good enough, or may be too slow.\n",
    "\n",
    "In this case, you'll want to form your own **pipeline**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94134\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\") # creates a blank model, specifying english\n",
    "\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "s = requests.get(\"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\")\n",
    "soup = BeautifulSoup(s.content).text.replace(\"-\\n\", \"\").replace(\"\\n\", \" \")\n",
    "nlp.max_length = 5278439\n",
    "\n",
    "doc = nlp(soup)\n",
    "print (len(list(doc.sents))) # 94k sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining a pipeline\n",
    "\n",
    "In spaCy, we have a few different ways to study a pipeline. If we want to do this in a script, we can do the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []}}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = load_nlp(\"en_core_web_sm\")\n",
    "\n",
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the **dictionary structure**. This tells us not only what is inside the pipeline, but its order. \n",
    "\n",
    "Each **key** after “summary” is a `pipe`. The **value** is a dictionary. This dictionary tells us a few different things. \n",
    "\n",
    "All of these value dictionaries state: `“assigns”` which corresponds to a value of what that particular pipe assigns to the token and doc as it passes through the pipeline. \n",
    "\n",
    "In some cases, there will be a key of “scores” in the dictionary. This indicates how the machine learning model was evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

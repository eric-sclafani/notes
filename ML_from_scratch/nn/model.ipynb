{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets from Scratch\n",
    "\n",
    "This notebook contains material on how to implement feed forward neural nets from scratch, following the book named as such: https://nnfs.io/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "nnfs.init()\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "When data forward-propogates through the network, it passes through successive `layers`.\n",
    "\n",
    "\n",
    "## Dense layers\n",
    "\n",
    "In  **`Dense layers`**, each neuron in a layer receives the **weighted sum** of the `inputs` and `weights` from the previous layer.\n",
    "\n",
    "- Each data point has a weight assigned to it. So: `len(input) = len(weights)`\n",
    "\n",
    "For each layer, the **input size must match the output size** from the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \n",
    "        # initialize random weights. \n",
    "        # The shape is (input size, # of neurons) instead of (# of neurons, input size) to avoid having to transpose later \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons) # 0.01 scales the weights down for faster training\n",
    "        \n",
    "        # initialize biases as zero vectors\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "         \n",
    "    def forward(self, inputs):\n",
    "        # weighted sum\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        \n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "\n",
    "`Activation functions` are used to apply transformations (mostly non-linear) to our input data.\n",
    "\n",
    "Most activations are `non-linear` because we want to solve `non-linear` problems.\n",
    "\n",
    "## Sigmoid\n",
    "\n",
    "`Sigmoid` is known as the **squashification** function because it reduces all values into a range of [0,1]\n",
    "\n",
    "Today, `sigmoid` is not used much inside of hidden layers, but sometimes can be found in the output layer for classification tasks.\n",
    "\n",
    "<font size=\"5\"> $y = \\frac{1}{1+ e^{-x}}$ </font>\n",
    "\n",
    "\n",
    "## ReLU\n",
    "\n",
    "`ReLU` is super simple: it takes the **max** between 0 and the input data.\n",
    "\n",
    "`ReLU` is the **most widely used activation** in hidden layers, mainly because of its speed and efficiency\n",
    "\n",
    "$$ y =   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      x & x>0 \\\\\n",
    "      0 & x \\le 0\\\\\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    "\n",
    "## Softmax\n",
    "\n",
    "`Softmax`, like sigmoid, squishes values into [0,1]. However, it outputs a `probability distribution`, meaning all the output values sum to 1\n",
    "\n",
    "The output values can also be thought of as **`confidence scores`**:\n",
    "\n",
    "- `Higher probability` = higher confidence the model has \n",
    "- `Lower probability` = lower confidence the model has \n",
    "\n",
    "<font size=\"5\">$s_{i,j} = \\frac{e^{z_{i,j}}}{\\sum_{l=1}^L e^{z_{i,j}}}$</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"Some notes about ReLU:\n",
    "    1. ReLU is not normalized, meaning values can range from [0,infinity]\n",
    "    2. ReLU outputs are completely independent of each other (exclusive)\n",
    "    3. Because of the two reasons above, ReLU cannot be used in the final layer for predicting probabilities (classification)\n",
    "    4. np.maximum takes the element wise max between two arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # any negative values are turned into 0\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "       \n",
    "class Softmax:\n",
    "    \"\"\"Some notes about softmax:\n",
    "    1. Softmax returns a probability distribution (all the floats add up to 1)\n",
    "    2. Each probability score also represents a confidence score (i.e., [.45, .55] means the model has low confidence)\n",
    "    3. Softmax is almost exclusively used in the output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        \"\"\"More notes:\n",
    "        1. axis=1 specifies that we should only operate across rows, not columns\n",
    "        \n",
    "        2. keepdims=True makes it so the output array has the same dimensions as the input\n",
    "        \n",
    "        3. we subtract the largest of the inputs to prevent \"dead neurons\" and exploding values.\n",
    "            - Dead neurons = when neurons start always outputting a specific value and thus have a zero gradient\n",
    "            - exploding values = when values start getting exponentially large\n",
    "            \n",
    "        4. performing this subtraction scales the values to a range [-1,0]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        # normalize them for each sample\n",
    "        probs = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probs\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            \n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            \n",
    "            # calculate jacobian matrix of the output \n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            \n",
    "            # calculate sample-wise gradient\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "\n",
    "`Loss functions` (also knows as **cost functions**) are the mechanism that tells our ML algorithms how wrong the predictions were.\n",
    "\n",
    "Loss functions are different depending on whether you're doing **regression** or **classification**\n",
    "\n",
    "Because neural networks output `confidence levels`, simply taking the **argmax** of an output vector will not suffice.\n",
    "\n",
    "\n",
    "## Categorical Cross Entropy Loss\n",
    "\n",
    "`Categorical Cross Entropy` (or **CCE**) is used to compare a `target probability distribution` ($y$) and some `predicted probability distribution` ($\\hat{y}$).\n",
    "\n",
    "Often used as loss when **softmax** is in the output layer.\n",
    "\n",
    "<font size=\"5\"> $L_i = -\\sum_j y_{i,j} log(\\hat{y}_{i,j})$</font>\n",
    "\n",
    "where:\n",
    "- $L_i$ = sample loss value\n",
    "- $i$ = $i$-th sample\n",
    "- $j$ = label/output index\n",
    "- $y$ = target values\n",
    "- $\\hat{y}$ = predicted values\n",
    "\n",
    "\n",
    "The equation above, however, can be simplified to:\n",
    "\n",
    "<font size=\"5\"> $L_i = -log(\\hat{y}_{i,k})$</font>\n",
    "\n",
    "where:\n",
    "- $k$ = index of the \"true\" probability\n",
    "\n",
    "We can simplify to this equation because the targets (in this case) are `sparse vectors` or `sparse matrices` (one-hot).\n",
    "\n",
    "This means our targets will only ever be 1 or 0, allowing us to make simplifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \n",
    "    def calculate(self, output, y):\n",
    "        \n",
    "        # calculate the sample losses\n",
    "        sample_losses = self.forward(output,y)\n",
    "        \n",
    "        # calculate the mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        return data_loss\n",
    "    \n",
    "class CategoricalCrossEntropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        # number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # np.clip will turn every value under the minimum into a_min, and every value over the maximum to a_max\n",
    "        \n",
    "        # clip data to prevent division by zero\n",
    "        # clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, a_min=1e-7, a_max=1-1e-7)\n",
    "        \n",
    "        \n",
    "        #! probabilities for target values:\n",
    "        \n",
    "        # only if categorical labels (1D one-hot vectors)\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        \n",
    "        # only for one-hot matrices\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "            \n",
    "            \n",
    "        # losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0]) # number of labels in every sample.\n",
    "        \n",
    "        # if labels are sparse, make them one-hot\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        # calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        \n",
    "        # normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        \n",
    "class Softmax_with_CCE_loss():\n",
    "    \"\"\"This is softmax combined with CCE unser one constructor. This enables faster backward passes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.softmax = Softmax()\n",
    "        self.cce = CategoricalCrossEntropy()\n",
    "        \n",
    "    def forward(self, inputs, y_true):\n",
    "        \n",
    "        self.softmax.forward(inputs)\n",
    "        \n",
    "        self.output = self.softmax.output\n",
    "        \n",
    "        return self.cce.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        \n",
    "        self.dinputs = self.dinputs / samples\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "After **backpropagation** calculates the gradients of all the functions in the network, the next step is for the `optimizer` to adjust the parameters (weights and biases) of the network.\n",
    "\n",
    "The most commomnly used, albeit possibly outdated optimizer is called **`Stochastic Gradient Descent`** or SGD. \n",
    "\n",
    "Most optimizers used today are just **variants of SGD**\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "`SGD` is an optimizer that processes either a single or multiple samples at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \n",
    "    def __init__(self, lr = 1.0):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        \"\"\"Multiplies the negated lr with the gradients stored in the layers and adds the result to the layer's params\n",
    "        \n",
    "           We negate the lr because we want to go in the opposite direction of the gradient\"\"\"\n",
    "        layer.weights += -self.lr * layer.dweights\n",
    "        layer.biases += -self.lr * layer.dbiases\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code execution\n",
    "The following cells are for executing and testing the neural network code\n",
    "\n",
    "-----\n",
    "\n",
    "The cell below combines what we've done so far:\n",
    "\n",
    "- Gather our data\n",
    "- Instantiate two hidden layers\n",
    "- Assign `ReLU` to **layer 1** and `softmax` to **layer 2**\n",
    "- Feed our data through each layer\n",
    "- Feed the softmax output into the CategoricalCrossEntropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Accuracy: 0.3433333333333333\n",
      "Loss: 1.0986183881759644\n",
      "Epoch: 100\n",
      "Accuracy: 0.4066666666666667\n",
      "Loss: 1.083395004272461\n",
      "Epoch: 200\n",
      "Accuracy: 0.39666666666666667\n",
      "Loss: 1.0709291696548462\n",
      "Epoch: 300\n",
      "Accuracy: 0.41\n",
      "Loss: 1.069575548171997\n",
      "Epoch: 400\n",
      "Accuracy: 0.41333333333333333\n",
      "Loss: 1.0685349702835083\n",
      "Epoch: 500\n",
      "Accuracy: 0.41333333333333333\n",
      "Loss: 1.0667901039123535\n",
      "Epoch: 600\n",
      "Accuracy: 0.41\n",
      "Loss: 1.0637015104293823\n",
      "Epoch: 700\n",
      "Accuracy: 0.4266666666666667\n",
      "Loss: 1.0575783252716064\n",
      "Epoch: 800\n",
      "Accuracy: 0.44666666666666666\n",
      "Loss: 1.04678213596344\n",
      "Epoch: 900\n",
      "Accuracy: 0.4033333333333333\n",
      "Loss: 1.0477620363235474\n",
      "Epoch: 1000\n",
      "Accuracy: 0.39666666666666667\n",
      "Loss: 1.0402601957321167\n",
      "Epoch: 1100\n",
      "Accuracy: 0.42\n",
      "Loss: 1.032832384109497\n",
      "Epoch: 1200\n",
      "Accuracy: 0.44666666666666666\n",
      "Loss: 1.0229641199111938\n",
      "Epoch: 1300\n",
      "Accuracy: 0.47333333333333333\n",
      "Loss: 1.0182338953018188\n",
      "Epoch: 1400\n",
      "Accuracy: 0.44\n",
      "Loss: 1.016717553138733\n",
      "Epoch: 1500\n",
      "Accuracy: 0.4633333333333333\n",
      "Loss: 1.0010089874267578\n",
      "Epoch: 1600\n",
      "Accuracy: 0.43333333333333335\n",
      "Loss: 0.9996526837348938\n",
      "Epoch: 1700\n",
      "Accuracy: 0.5033333333333333\n",
      "Loss: 0.9816291332244873\n",
      "Epoch: 1800\n",
      "Accuracy: 0.42\n",
      "Loss: 0.9849529266357422\n",
      "Epoch: 1900\n",
      "Accuracy: 0.5066666666666667\n",
      "Loss: 0.9812688231468201\n",
      "Epoch: 2000\n",
      "Accuracy: 0.4666666666666667\n",
      "Loss: 0.9579774141311646\n",
      "Epoch: 2100\n",
      "Accuracy: 0.48333333333333334\n",
      "Loss: 0.9578478932380676\n",
      "Epoch: 2200\n",
      "Accuracy: 0.4866666666666667\n",
      "Loss: 0.946531355381012\n",
      "Epoch: 2300\n",
      "Accuracy: 0.45666666666666667\n",
      "Loss: 0.9837367534637451\n",
      "Epoch: 2400\n",
      "Accuracy: 0.48333333333333334\n",
      "Loss: 0.9414766430854797\n",
      "Epoch: 2500\n",
      "Accuracy: 0.5233333333333333\n",
      "Loss: 0.9196807742118835\n",
      "Epoch: 2600\n",
      "Accuracy: 0.53\n",
      "Loss: 0.8560364842414856\n",
      "Epoch: 2700\n",
      "Accuracy: 0.6233333333333333\n",
      "Loss: 0.8208872675895691\n",
      "Epoch: 2800\n",
      "Accuracy: 0.5933333333333334\n",
      "Loss: 0.8317890167236328\n",
      "Epoch: 2900\n",
      "Accuracy: 0.6633333333333333\n",
      "Loss: 0.7829446196556091\n",
      "Epoch: 3000\n",
      "Accuracy: 0.62\n",
      "Loss: 0.7760156393051147\n",
      "Epoch: 3100\n",
      "Accuracy: 0.63\n",
      "Loss: 0.7971696853637695\n",
      "Epoch: 3200\n",
      "Accuracy: 0.6533333333333333\n",
      "Loss: 0.7605178356170654\n",
      "Epoch: 3300\n",
      "Accuracy: 0.57\n",
      "Loss: 0.7596697211265564\n",
      "Epoch: 3400\n",
      "Accuracy: 0.62\n",
      "Loss: 0.7879543304443359\n",
      "Epoch: 3500\n",
      "Accuracy: 0.6466666666666666\n",
      "Loss: 0.7306738495826721\n",
      "Epoch: 3600\n",
      "Accuracy: 0.6466666666666666\n",
      "Loss: 0.7045900225639343\n",
      "Epoch: 3700\n",
      "Accuracy: 0.6966666666666667\n",
      "Loss: 0.6559889912605286\n",
      "Epoch: 3800\n",
      "Accuracy: 0.6066666666666667\n",
      "Loss: 0.6692435145378113\n",
      "Epoch: 3900\n",
      "Accuracy: 0.6633333333333333\n",
      "Loss: 0.6671760082244873\n",
      "Epoch: 4000\n",
      "Accuracy: 0.69\n",
      "Loss: 0.6560653448104858\n",
      "Epoch: 4100\n",
      "Accuracy: 0.6966666666666667\n",
      "Loss: 0.650726318359375\n",
      "Epoch: 4200\n",
      "Accuracy: 0.6566666666666666\n",
      "Loss: 0.7336700558662415\n",
      "Epoch: 4300\n",
      "Accuracy: 0.7033333333333334\n",
      "Loss: 0.634941577911377\n",
      "Epoch: 4400\n",
      "Accuracy: 0.58\n",
      "Loss: 1.0367368459701538\n",
      "Epoch: 4500\n",
      "Accuracy: 0.7233333333333334\n",
      "Loss: 0.6218185424804688\n",
      "Epoch: 4600\n",
      "Accuracy: 0.71\n",
      "Loss: 0.6258591413497925\n",
      "Epoch: 4700\n",
      "Accuracy: 0.7466666666666667\n",
      "Loss: 0.6018945574760437\n",
      "Epoch: 4800\n",
      "Accuracy: 0.73\n",
      "Loss: 0.6026360988616943\n",
      "Epoch: 4900\n",
      "Accuracy: 0.7333333333333333\n",
      "Loss: 0.6013962030410767\n",
      "Epoch: 5000\n",
      "Accuracy: 0.59\n",
      "Loss: 1.0101546049118042\n",
      "Epoch: 5100\n",
      "Accuracy: 0.7533333333333333\n",
      "Loss: 0.591923713684082\n",
      "Epoch: 5200\n",
      "Accuracy: 0.7366666666666667\n",
      "Loss: 0.5954970717430115\n",
      "Epoch: 5300\n",
      "Accuracy: 0.7466666666666667\n",
      "Loss: 0.5875529646873474\n",
      "Epoch: 5400\n",
      "Accuracy: 0.7133333333333334\n",
      "Loss: 0.5940183401107788\n",
      "Epoch: 5500\n",
      "Accuracy: 0.5833333333333334\n",
      "Loss: 1.0905221700668335\n",
      "Epoch: 5600\n",
      "Accuracy: 0.7633333333333333\n",
      "Loss: 0.5763997435569763\n",
      "Epoch: 5700\n",
      "Accuracy: 0.7466666666666667\n",
      "Loss: 0.5806926488876343\n",
      "Epoch: 5800\n",
      "Accuracy: 0.7433333333333333\n",
      "Loss: 0.5793994069099426\n",
      "Epoch: 5900\n",
      "Accuracy: 0.7366666666666667\n",
      "Loss: 0.5793479084968567\n",
      "Epoch: 6000\n",
      "Accuracy: 0.7366666666666667\n",
      "Loss: 0.5803730487823486\n",
      "Epoch: 6100\n",
      "Accuracy: 0.74\n",
      "Loss: 0.5823854207992554\n",
      "Epoch: 6200\n",
      "Accuracy: 0.74\n",
      "Loss: 0.5820953249931335\n",
      "Epoch: 6300\n",
      "Accuracy: 0.74\n",
      "Loss: 0.5818667411804199\n",
      "Epoch: 6400\n",
      "Accuracy: 0.74\n",
      "Loss: 0.5793887972831726\n",
      "Epoch: 6500\n",
      "Accuracy: 0.74\n",
      "Loss: 0.5774703621864319\n",
      "Epoch: 6600\n",
      "Accuracy: 0.74\n",
      "Loss: 0.574521005153656\n",
      "Epoch: 6700\n",
      "Accuracy: 0.74\n",
      "Loss: 0.5722926259040833\n",
      "Epoch: 6800\n",
      "Accuracy: 0.74\n",
      "Loss: 0.5713061690330505\n",
      "Epoch: 6900\n",
      "Accuracy: 0.7466666666666667\n",
      "Loss: 0.5677348971366882\n",
      "Epoch: 7000\n",
      "Accuracy: 0.75\n",
      "Loss: 0.5632905960083008\n",
      "Epoch: 7100\n",
      "Accuracy: 0.7733333333333333\n",
      "Loss: 0.5594690442085266\n",
      "Epoch: 7200\n",
      "Accuracy: 0.7966666666666666\n",
      "Loss: 0.5449924468994141\n",
      "Epoch: 7300\n",
      "Accuracy: 0.5666666666666667\n",
      "Loss: 1.3461910486221313\n",
      "Epoch: 7400\n",
      "Accuracy: 0.75\n",
      "Loss: 0.5865427851676941\n",
      "Epoch: 7500\n",
      "Accuracy: 0.76\n",
      "Loss: 0.5717695951461792\n",
      "Epoch: 7600\n",
      "Accuracy: 0.7533333333333333\n",
      "Loss: 0.5673360824584961\n",
      "Epoch: 7700\n",
      "Accuracy: 0.7533333333333333\n",
      "Loss: 0.5643007755279541\n",
      "Epoch: 7800\n",
      "Accuracy: 0.76\n",
      "Loss: 0.559775173664093\n",
      "Epoch: 7900\n",
      "Accuracy: 0.7666666666666667\n",
      "Loss: 0.5539730191230774\n",
      "Epoch: 8000\n",
      "Accuracy: 0.6366666666666667\n",
      "Loss: 0.8100103139877319\n",
      "Epoch: 8100\n",
      "Accuracy: 0.76\n",
      "Loss: 0.5741829872131348\n",
      "Epoch: 8200\n",
      "Accuracy: 0.7633333333333333\n",
      "Loss: 0.5613110065460205\n",
      "Epoch: 8300\n",
      "Accuracy: 0.7733333333333333\n",
      "Loss: 0.5537437796592712\n",
      "Epoch: 8400\n",
      "Accuracy: 0.8\n",
      "Loss: 0.5440529584884644\n",
      "Epoch: 8500\n",
      "Accuracy: 0.7166666666666667\n",
      "Loss: 0.6415289640426636\n",
      "Epoch: 8600\n",
      "Accuracy: 0.76\n",
      "Loss: 0.5601934790611267\n",
      "Epoch: 8700\n",
      "Accuracy: 0.7733333333333333\n",
      "Loss: 0.5500548481941223\n",
      "Epoch: 8800\n",
      "Accuracy: 0.7866666666666666\n",
      "Loss: 0.5431088209152222\n",
      "Epoch: 8900\n",
      "Accuracy: 0.5\n",
      "Loss: 2.030160427093506\n",
      "Epoch: 9000\n",
      "Accuracy: 0.77\n",
      "Loss: 0.5557321310043335\n",
      "Epoch: 9100\n",
      "Accuracy: 0.78\n",
      "Loss: 0.5484169721603394\n",
      "Epoch: 9200\n",
      "Accuracy: 0.8033333333333333\n",
      "Loss: 0.5344687104225159\n",
      "Epoch: 9300\n",
      "Accuracy: 0.7733333333333333\n",
      "Loss: 0.5543602705001831\n",
      "Epoch: 9400\n",
      "Accuracy: 0.7766666666666666\n",
      "Loss: 0.550236165523529\n",
      "Epoch: 9500\n",
      "Accuracy: 0.7866666666666666\n",
      "Loss: 0.5405285358428955\n",
      "Epoch: 9600\n",
      "Accuracy: 0.7666666666666667\n",
      "Loss: 0.5633888840675354\n",
      "Epoch: 9700\n",
      "Accuracy: 0.78\n",
      "Loss: 0.5463889241218567\n",
      "Epoch: 9800\n",
      "Accuracy: 0.7866666666666666\n",
      "Loss: 0.5404762625694275\n",
      "Epoch: 9900\n",
      "Accuracy: 0.7666666666666667\n",
      "Loss: 0.564641535282135\n",
      "Epoch: 10000\n",
      "Accuracy: 0.78\n",
      "Loss: 0.5441303253173828\n"
     ]
    }
   ],
   "source": [
    "X,y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = DenseLayer(2,64)\n",
    "relu = ReLU()\n",
    "\n",
    "dense2 = DenseLayer(64,3)\n",
    "softmax = Softmax()\n",
    "\n",
    "loss_activation = Softmax_with_CCE_loss()\n",
    "\n",
    "optimizer = SGD()\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # layer 1 forward pass\n",
    "    dense1.forward(X)\n",
    "    relu.forward(dense1.output)\n",
    "\n",
    "    # layer 2 forward pass\n",
    "    dense2.forward(relu.output)\n",
    "\n",
    "    # forward pass through second layer activation function\n",
    "    # and through the loss function\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # calculate accuracy from output of softmax and targets\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "        \n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100: # prints every 100 epochs (everytime epoch % 100 equals 0)\n",
    "        print(f\"Epoch: {epoch}\\nAccuracy: {accuracy}\\nLoss: {loss}\")\n",
    "\n",
    "\n",
    "    # backward pass (a.k.a backpropagation)\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    relu.backward(dense2.dinputs)\n",
    "    dense1.backward(relu.dinputs)\n",
    "\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

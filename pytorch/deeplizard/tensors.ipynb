{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "`Tensors` are the backbone of neural network programming. The **inputs**, **outputs**, and **tranformations** are all represented using `tensors`. \n",
    "\n",
    "By definition, a `tensor` is a mathematical object that holds $n$-dimensional data.  \n",
    "\n",
    "The term itself is a generalization of multiple terms referring to the same concept. People will refer to them differently depending on their background:\n",
    "\n",
    "- Mathematicians: `scalar, vector, matrix`\n",
    "- Computer scientists: `0d-array, 1d-array, 2d-array`\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "# Tensor creation\n",
    "\n",
    "All the **factory functions** have an optional argument `dtype`, where we can directly specify the tensor type.\n",
    "\n",
    "(**Note**: factory functions accept parameter inputs and returns a particular type of object)\n",
    "\n",
    "## Tensor creation with data\n",
    "**The first two ways `create a copy of the data`. This means they create tensors that occupy different memory addresses**\n",
    "\n",
    "1. `torch.Tensor(data)` - This is the class constructor for tensors. **All tensors are instances of this class.** Calling the constructor directly is not typically done because of its limitations (no dtype arg, etc...)\n",
    "\n",
    "2. `torch.tensor(data)` - The **main** factory function that creates Tensor objects for us. `<--Generally, this is the go-to option for making tensors`\n",
    "\n",
    "**The last two ways `share the same memory as the data itself`. This can be memory efficient, but can have unintended side affects (`mutating data == mutating tensor (and vice versa)`)**\n",
    "\n",
    "3. `torch.as_tensor(data)` - Also a factory function. Creates a tensor object.\n",
    "\n",
    "4. `torch.from_numpy(data)` - Also a factory function. Creates a tensor from a numpy array.\n",
    "\n",
    "\n",
    "**We can also convert a tensor into a numpy array by using the `.numpy` tensor method**\n",
    "```python\n",
    ">>> torch.tensor([[1,2,3]]).numpy()\n",
    "array([[1,2,3]])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Tensor creation without data\n",
    "\n",
    "**There exists many methods of instantiating tensors without data, such as (but not limited to)**:\n",
    "\n",
    "-------\n",
    "1. `torch.eye()` - returns the identity matrix\n",
    "```python\n",
    ">>> torch.eye(2)\n",
    "tensor([[1., 0.],\n",
    "        [0., 1.]])\n",
    "```\n",
    "-------\n",
    "2. `torch.zeros()` - returns a tensor of zeros of specified shape\n",
    "```python\n",
    ">>> torch.zeros(2,2)\n",
    "tensor([[0., 0.],\n",
    "        [0., 0.]])\n",
    "```\n",
    "-------\n",
    "3. `torch.ones()` - returns a tensor of ones of specified shape\n",
    "```python\n",
    ">>> torch.ones(3,3)\n",
    "tensor([[1., 1., 1.],\n",
    "        [1., 1., 1.],\n",
    "        [1., 1., 1.,]])\n",
    "```\n",
    "-------\n",
    "4. `tensor.fill_()` - fills an already existing tensor with a specified value\n",
    "```python\n",
    ">>> t = torch.ones(2,2)\n",
    ">>> t.fill_(5)\n",
    "tensor([[5,5],\n",
    "        [5,5]])\n",
    "```\n",
    "-------\n",
    "5. `torch.rand()` - returns a tensor of random floats from a `uniform distribution` (a flat line)\n",
    "```python\n",
    ">>> torch.rand(2,2)\n",
    "tensor([[0.9219, 0.5228],\n",
    "        [0.8266, 0.2281]])\n",
    "```\n",
    "-------\n",
    "6. `torch.randn()` - returns a tensor of random floats from a `normal distribution` (a gaussian curve)\n",
    "```python\n",
    ">>> torch.randn(2,2)\n",
    "tensor([[ 0.8713, -0.0966],\n",
    "        [ 2.4263,  0.3114]])\n",
    "```\n",
    "-------\n",
    "7. `torch.randint()` - returns a tensor of random integers.\n",
    "```python\n",
    ">>> torch.randint(1,100, (2,2))\n",
    "tensor([[ 7, 32],\n",
    "        [19, 87]])\n",
    "```\n",
    "-------\n",
    "8. `torch.arange()` - returns a tensor of integers from a given span\n",
    "```python\n",
    ">>> torch.arange(1,10,2)\n",
    "tensor([1, 3, 5, 7, 9])\n",
    "```\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch tensor attributes\n",
    "\n",
    "There are numerous pytorch-specific attributes that torch tensors have:\n",
    "```python\n",
    ">>> t = torch.tensor([[1,2,3]])\n",
    "```\n",
    "## .dtype \n",
    "Returns the **data type** of the tensor\n",
    "Tensor operations must occur with **tenors of the same data type**\n",
    "```python\n",
    ">>> t.dtype\n",
    "torch.float32\n",
    "```\n",
    "## .device \n",
    "Returns which device (cpu or cuda) the tensor's data is allocated\n",
    "\n",
    "Tensor operations must occur with **tensors on the same device**\n",
    "```python\n",
    ">>> t.device\n",
    "cpu\n",
    ">>> device = torch.device(\"cuda:0\")\n",
    "device(type=\"cuda\", index=0) # cuda supports multiple devices, hence the indexing\n",
    "\n",
    ">>> tensor = torch.tensor([[1,2,3]], device=device)\n",
    "tensor([[1,2,3]], device=\"cuda:0\")\n",
    "\n",
    "```\n",
    "## .layout\n",
    "\n",
    "Returns how out tensor's data is stored in memory\n",
    "```python\n",
    ">>> tensor = torch.tensor([[1,2,3]])\n",
    ">>> tensor.layout\n",
    "torch.strided\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank, Axes, and Shape\n",
    "\n",
    "## Rank\n",
    "\n",
    "The `rank` of a tensor refers to the number of dimensions present within the tensor. Suppose we have a rank-2 tensor. This means the following:\n",
    "- We have a **matrix**\n",
    " - We have a 2d-array\n",
    "- we have a 2d-tensor\n",
    "    \n",
    "The `rank` tells us how many **indices** are required to access a specific data element in the tensor:\n",
    "```python\n",
    ">>> a = torch.tensor([1,2,3,4]) \n",
    ">>> a[0] <--- Rank 1 Tensor\n",
    "1 \n",
    "\n",
    ">>> b = torch.tensor([[1,2,3,4], [5,6,7,8]]) \n",
    ">>> b[1][0]\n",
    "5 <--- Rank 2 Tensor \n",
    "```\n",
    "\n",
    "## Axes\n",
    "\n",
    "An `axis` of a tensor is a specific dimension. For example, a **rank-2** tensor has 2 dimensions, or 2 `axes`\n",
    "\n",
    "Elements are said to **exist** or **run** along a particular axis. The `length of each axis` tells us **how many indices are available along the axis**\n",
    "\n",
    "Consider the tensor: \n",
    "```python\n",
    ">>> t = torch.tensor([[1,2,3], [4,5,6],[7,8,9]])\n",
    "\n",
    "```\n",
    "Each element along the first axis is an **array** (1d-tensor), and each element along the second axis is a **scalar** (0d-tensor)\n",
    "\n",
    "```python\n",
    ">>> t[0]\n",
    "tensor([1,2,3])\n",
    ">>> t[1]\n",
    "tensor([4,5,6])\n",
    ">>> t[2]\n",
    "tensor([7,8,9])\n",
    ">>> t[0][0]\n",
    "1\n",
    ">>> t[1][0]\n",
    "4\n",
    ">>> t[2][0]\n",
    "7\n",
    ">>> t[0][1]\n",
    "2\n",
    ">>> t[1][1]\n",
    "5\n",
    ">>> t[2][1]\n",
    "8\n",
    ">>> t[0][2]\n",
    "3\n",
    ">>> t[1][2]\n",
    "6\n",
    ">>> t[2][2]\n",
    "9\n",
    "```\n",
    "\n",
    "## Shape\n",
    "\n",
    "The `shape` of a tensor is determined by the length of each axis. The `shape` tells us how many indices are available along each axis.\n",
    "\n",
    "**The `rank` of a tensor is equal to the length of its `shape`**\n",
    "\n",
    "Consider the tensor: \n",
    "```python\n",
    ">>> t = torch.tensor([[1,2,3,4],[4,5,6,7],[7,8,9,10]])\n",
    ">>> t.shape\n",
    "torch.Size([3,4])\n",
    ">>> t.size()\n",
    "torch.Size([3,4])\n",
    ">>> len(t.shape) # <--- Gives us the rank\n",
    "2\n",
    "```\n",
    "\n",
    "In the above tensor, we have **3 rows** and **4 columns**. Therefore, we can index a total of 3 times for each axis.\n",
    "\n",
    "**The `product` of the `shape values`** == **# of `elements in tensor`**\n",
    "```python\n",
    ">>> torch.tensor(t.shape).prod()\n",
    "tensor(12)\n",
    "```\n",
    "We can check **how many elements are in a tensor** with the `.numel` method\n",
    "```python\n",
    ">>> t.numel()\n",
    "12\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor operations\n",
    "\n",
    "## Reshaping operations\n",
    "\n",
    "Often times we need to **`change the shape`** of our tensors **without mutating the data within them**. This is because as data propogates through a NN, different layers expect different shapes from tensors.\n",
    "\n",
    "-------\n",
    "### Reshape\n",
    "\n",
    "The `.reshape()` method lets us do this. We can take a tensor and reshape it into a specified shape **as long as the product of the reshaping values == the amount of items in the tensor**.\n",
    "\n",
    "(Note: the `.view()` method essentially does the same thing, but has slightly different behaviour. In general, use `.reshape()` instead of view.)\n",
    "```python\n",
    ">>> t = torch.tensor([[1,1,1,1], [2,2,2,2],[3,3,3,3]])\n",
    "\n",
    ">>> t.reshape(1,12)\n",
    "tensor([[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]])\n",
    "\n",
    ">>> t.reshape(2,6)\n",
    "tensor([[1, 1, 1, 1, 2, 2],\n",
    "        [2, 2, 3, 3, 3, 3]])\n",
    "\n",
    ">>> t.reshape(6,2)\n",
    "tensor([[1, 1],\n",
    "        [1, 1],\n",
    "        [2, 2],\n",
    "        [2, 2],\n",
    "        [3, 3],\n",
    "        [3, 3]])\n",
    ">>> t.reshape(3,4)\n",
    "tensor([[1, 1, 1, 1],\n",
    "        [2, 2, 2, 2],\n",
    "        [3, 3, 3, 3]])\n",
    "\n",
    ">>> t.reshape(4,3)\n",
    "tensor([[1, 1, 1],\n",
    "        [1, 2, 2],\n",
    "        [2, 2, 3],\n",
    "        [3, 3, 3]])\n",
    "\n",
    ">>> t.reshape(12,1)\n",
    "tensor([[1],\n",
    "        [1],\n",
    "        [1],\n",
    "        [1],\n",
    "        [2],\n",
    "        [2],\n",
    "        [2],\n",
    "        [2],\n",
    "        [3],\n",
    "        [3],\n",
    "        [3],\n",
    "        [3]])\n",
    "```\n",
    "------\n",
    "### Squeeze and unsqueeze\n",
    "\n",
    "`.squeeze()` removes all axes that have a length of 1\n",
    "```python\n",
    ">>> t.reshape(1,12).squeeze()\n",
    "tensor([1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3])\n",
    "\n",
    "```\n",
    "\n",
    "`.unsqueeze()` adds a dimension with length of 1 to a specified axis\n",
    "```python\n",
    ">>> t.reshape(1,12).squeeze().unsqueeze(dim=0)\n",
    "tensor([[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]])\n",
    "```\n",
    "------\n",
    "\n",
    "### Flatten\n",
    "\n",
    "To `flatten` a tensor means to turn it into a lower rank tensor, or 1-dimensional tensor. Flattening must be done when transitioning from a convolutional layer to a fully-connected one.\n",
    "\n",
    "Pytorch has a `.flatten()` method that does just that, but here is an implementation using previous methods:\n",
    "\n",
    "```python\n",
    "\n",
    "def flatten(t):\n",
    "        t = t.reshape(1,-1) # the -1 tells reshape to figure out what the value should be based on the other element and # of elements in the tensor (think python [-1] indexing)\n",
    "        t = t.squeeze()\n",
    "        return t\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Element-wise operations\n",
    "`Element-wise operations` are operations between two tensors that operate over elements that share corresponding indices between the two tensors.\n",
    "\n",
    "Other names for element wise include: `component-wise` and `point-wise`\n",
    "\n",
    "**Tensors must have the `same shape` to be operated upon.**\n",
    "\n",
    "Basic arithmetic operations between tensors are element-wise: **addition, subtraction, multiplication, division**\n",
    "\n",
    "```python\n",
    "\n",
    ">>> t1 = torch.tensor([[1,2], [3,4])\n",
    ">>> t2 = torch.tensor([[9,8], [7,6]])\n",
    "\n",
    ">>> t1 + t2\n",
    "torch.tensor([[10,10], [10,10]])\n",
    "\n",
    ">>> torch.add(t1,t2)\n",
    "torch.tensor([[10,10], [10,10]])\n",
    "```\n",
    "\n",
    "In the code snippet above, all the other arithmetic operations follow the same syntax.\n",
    "\n",
    "### Broadcasting\n",
    "\n",
    "**`Broadcasting`** means reshaping a smaller tensor to match the shape of a larger one.\n",
    "\n",
    "Often times, we need to perform an arithmetic operation `between a tensor and a scalar`.\n",
    " \n",
    "In the code snippet below, the scalar values (a.k.a 0-d tensors) are being `broadcasted` into a 2-dimensional tensor before the operation is performed\n",
    "```python\n",
    ">>> t1 - 2\n",
    "tensor([[-1,0], [1,2]])\n",
    "\n",
    ">>> t1.div(2)\n",
    "tensor([[0.5000, 1.0000], [1.5000, 2.0000]])\n",
    "```\n",
    "\n",
    "\n",
    "We can also use numpy's `np.broadcast_to()` function to check what a scalar value is being broadcasted to.\n",
    "```python\n",
    ">>> np.broadcast_to(2, t1.shape)\n",
    "array([[2,2], [2,2]])\n",
    "```\n",
    "\n",
    "**`Comparison operations`** are also **element wise**. When comparing a tensor and a scalar value, the scalar is **broadcasted** and then compared.\n",
    "\n",
    "## Reduction operations\n",
    "\n",
    "A **`Reduction`** operation on a tensor is an operation that reduces the number of elements contained within the tensor.\n",
    "\n",
    "`Reduction operations` let us perform operations on a **single tensor**\n",
    "\n",
    "```python\n",
    ">>> t = torch.tensor([[0,1,0], [2,0,2], [0,3,0]])\n",
    "\n",
    ">>> t.sum()\n",
    "tensor(8)\n",
    "\n",
    ">>> t.prod()\n",
    "tensor(0)\n",
    "\n",
    ">>> t.numel() # number of elements\n",
    "9\n",
    "\n",
    ">>> t.sum().numel() < t.numel() # this case: 1 < 9>\n",
    "True\n",
    "\n",
    ">>> t.mean()\n",
    "tensor(0.8889)\n",
    "\n",
    ">>> t.std()\n",
    "tensor(1.1667)\n",
    "\n",
    ">>> t.max()\n",
    "tensor(3)\n",
    "\n",
    ">>> t.argmax() # returns index of highest value. Flattens the tensor before doing this. \n",
    "tensor(7)\n",
    "\n",
    ">>> t.min()\n",
    "tensor(0)\n",
    "\n",
    ">>> t.argmin()\n",
    "tensor(0)\n",
    "```\n",
    "To get the **scalar value** from a reduced tensor, we can use the `.item()` tensor method\n",
    "```python\n",
    "\n",
    ">>> t.max().item()\n",
    "3\n",
    "```\n",
    "\n",
    "We can also specify which **axes to perform the reduction**.\n",
    "```python\n",
    "\n",
    ">>> t = torch.tensor([[1,1,1,1], \n",
    "                      [2,2,2,2], \n",
    "                      [3,3,3,3]])\n",
    "\n",
    ">>> t.sum(dim=0)\n",
    "tensor([6, 6, 6, 6])\n",
    "\n",
    ">>> t.sum(dim=1)\n",
    "tensor([4, 8, 12])\n",
    "\n",
    ">>> t[0].sum()\n",
    "tensor(4)\n",
    "\n",
    ">>> t[1].sum()\n",
    "tensor(8)\n",
    "\n",
    ">>> t[2].sum()\n",
    "tensor(12)\n",
    "\n",
    ">>> t.max(dim=0)\n",
    "torch.return_types.max(values=tensor([3, 3, 3, 3]), indices=tensor([2, 2, 2, 2]))\n",
    "\n",
    ">>> t.argmax(dim=0)\n",
    "tensor([2, 2, 2, 2])\n",
    "\n",
    ">>> t.max(dim=1)\n",
    "torch.return_types.max(values=tensor([1, 2, 3]), indices=tensor([0, 0, 0]))\n",
    "\n",
    ">>> t.argmax(dim=1)\n",
    "tensor([0, 0, 0]) # index 0 of all three arrays\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74257a0a5c0181bc4827c74976dbddecd68f5367eb1980f961c7a318e63ecaee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
